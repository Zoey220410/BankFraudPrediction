{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91298ba9-865c-4530-8ed0-c73a13cfdeae",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a64000bc-3201-44a3-a4f1-269089ef2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Base.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.drop('fraud_bool')\n",
    "\n",
    "# Define preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define target and features\n",
    "X = data.drop('fraud_bool', axis=1)\n",
    "y = data['fraud_bool']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_prepared = preprocessor.fit_transform(X_train)\n",
    "X_test_prepared = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6238aed-8db5-413f-a168-8436ab87fbc2",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de6f6648-38f5-49c3-b020-633e520c3ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9889766666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_prepared, y_train)\n",
    "rf_predictions = rf_model.predict(X_test_prepared)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print('Random Forest Accuracy:', rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9b8f73f-f5dc-4dd1-859c-5fc1c3a5c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6bacbb-e87f-4230-bb15-f0c4c14a050f",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3351439b-26d1-43d0-a01b-cf3b941c3f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.9888566666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train_prepared, y_train)\n",
    "xgb_predictions = xgb_model.predict(X_test_prepared)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_predictions)\n",
    "print('XGBoost Accuracy:', xgb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5edc8a96-ee65-4b45-bc09-f39d2db24333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('xgboost_model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d55b7-02fb-49ee-bfda-e813b2303fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efd680c0-321a-43e4-b9ed-cbf5b16debfe",
   "metadata": {},
   "source": [
    "# Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04785067-6716-4a91-859f-564134f60e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 7720, number of negative: 692280\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3249\n",
      "[LightGBM] [Info] Number of data points in the train set: 700000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496176\n",
      "[LightGBM] [Info] Start training from score -4.496176\n",
      "LightGBM Accuracy: 0.98868\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm_model = LGBMClassifier(random_state=42)\n",
    "lgbm_model.fit(X_train_prepared, y_train)\n",
    "lgbm_predictions = lgbm_model.predict(X_test_prepared)\n",
    "lgbm_accuracy = accuracy_score(y_test, lgbm_predictions)\n",
    "print('LightGBM Accuracy:', lgbm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a561c8f-cef1-4d9f-bcff-741f8062e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lightgbm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lgbm_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c604e29c-8c27-4bdc-814b-93bdd78564c0",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da351c3a-32d3-40a5-9583-ce484cad7f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.98903\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_prepared, y_train)\n",
    "lr_predictions = lr_model.predict(X_test_prepared)\n",
    "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
    "print('Logistic Regression Accuracy:', lr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f9d932d-0cc9-46fd-9470-5b75991bb523",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logistic_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f398055-b5c4-4df0-b3c8-1d2fdfda19f9",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c44cf01-464c-4b51-b706-f12860966afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_names = list(preprocessor.transformers_[0][1].named_steps['scaler'].get_feature_names_out(numerical_cols)) + \\\n",
    "                list(preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_cols))\n",
    "\n",
    "\n",
    "rf_importances = rf_model.feature_importances_\n",
    "xgb_importances = xgb_model.feature_importances_\n",
    "lgbm_importances = lgbm_model.feature_importances_\n",
    "lr_importances = np.abs(lr_model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd4579a9-f7c1-4f1e-9d7b-471bd3254635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('housing_status_BA', 0.09746069950179924),\n",
       " ('name_email_similarity', 0.0449337856273705),\n",
       " ('credit_risk_score', 0.038271385787126404),\n",
       " ('device_os_windows', 0.036167628283688094),\n",
       " ('days_since_request', 0.03597900197247583)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_normalized = rf_importances / np.sum(rf_importances)\n",
    "xgb_normalized = xgb_importances / np.sum(xgb_importances)\n",
    "lgbm_normalized = lgbm_importances / np.sum(lgbm_importances)\n",
    "lr_normalized = lr_importances / np.sum(lr_importances)\n",
    "\n",
    "average_importances = (rf_normalized + xgb_normalized + lgbm_normalized + lr_normalized) / 4\n",
    "\n",
    "def get_top_n_features(importances, feature_names, n=5):\n",
    "    indices = np.argsort(importances)[-n:][::-1]\n",
    "    top_features = [(feature_names[i], importances[i]) for i in indices]\n",
    "    return top_features\n",
    "\n",
    "top5_overall = get_top_n_features(average_importances, feature_names)\n",
    "top5_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bee8d3fd-e821-47af-a855-8f7fbf462381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fraud_bool', 'income', 'name_email_similarity',\n",
       "       'prev_address_months_count', 'current_address_months_count',\n",
       "       'customer_age', 'days_since_request', 'intended_balcon_amount',\n",
       "       'payment_type', 'zip_count_4w', 'velocity_6h', 'velocity_24h',\n",
       "       'velocity_4w', 'bank_branch_count_8w',\n",
       "       'date_of_birth_distinct_emails_4w', 'employment_status',\n",
       "       'credit_risk_score', 'email_is_free', 'housing_status',\n",
       "       'phone_home_valid', 'phone_mobile_valid', 'bank_months_count',\n",
       "       'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source',\n",
       "       'session_length_in_minutes', 'device_os', 'keep_alive_session',\n",
       "       'device_distinct_emails_8w', 'device_fraud_count', 'month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9040df22-045f-44a6-a3b3-ac88eb968bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_feature_names = [feature[0] for feature in top5_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f173ab95-f7e2-4cc0-ae8c-12357cff1fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25281250117246007"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_sum = sum([feature[1] for feature in top5_overall])\n",
    "top5_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "203572ff-6d06-417c-ba41-5228f9122ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_feature_names = [\n",
    "'housing_status_BA',\n",
    " 'housing_status_BB',\n",
    " 'housing_status_BC',\n",
    " 'housing_status_BD',\n",
    " 'housing_status_BE',\n",
    " 'housing_status_BF',\n",
    " 'housing_status_BG',\n",
    " 'device_os_linux',\n",
    " 'device_os_macintosh',\n",
    " 'device_os_other',\n",
    " 'device_os_windows',\n",
    " 'device_os_x11',\n",
    " 'name_email_similarity',\n",
    " 'credit_risk_score',\n",
    " 'days_since_request'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d1b23-ce89-441c-8cbf-ac5de38b833e",
   "metadata": {},
   "source": [
    "# Retrain four models using only top 5 features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9b84a80b-51d6-4e7a-84e2-078e90f619e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train_top5 = X_train_prepared[:, [feature_names.index(f) for f in top5_feature_names]]\n",
    "X_test_top5 = X_test_prepared[:, [feature_names.index(f) for f in top5_feature_names]]\n",
    "# Initialize the models\n",
    "rf_model_top5 = RandomForestClassifier(random_state=42)\n",
    "xgb_model_top5 = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "lgbm_model_top5 = LGBMClassifier(random_state=42)\n",
    "lr_model_top5 = LogisticRegression(max_iter=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aebe8a00-be37-4e1b-b0af-43f80a0bdd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 7720, number of negative: 692280\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 789\n",
      "[LightGBM] [Info] Number of data points in the train set: 700000, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496176\n",
      "[LightGBM] [Info] Start training from score -4.496176\n",
      "RF Accuracy with Top 5 Features: 0.9888833333333333\n",
      "XGB Accuracy with Top 5 Features: 0.9889433333333333\n",
      "LGBM Accuracy with Top 5 Features: 0.9889166666666667\n",
      "LR Accuracy with Top 5 Features: 0.98897\n"
     ]
    }
   ],
   "source": [
    "# Fit the models on the reduced dataset\n",
    "rf_model_top5.fit(X_train_top5, y_train)\n",
    "xgb_model_top5.fit(X_train_top5, y_train)\n",
    "lgbm_model_top5.fit(X_train_top5, y_train)\n",
    "lr_model_top5.fit(X_train_top5, y_train)\n",
    "\n",
    "# Predict and evaluate the models\n",
    "rf_accuracy_top5 = accuracy_score(y_test, rf_model_top5.predict(X_test_top5))\n",
    "xgb_accuracy_top5 = accuracy_score(y_test, xgb_model_top5.predict(X_test_top5))\n",
    "lgbm_accuracy_top5 = accuracy_score(y_test, lgbm_model_top5.predict(X_test_top5))\n",
    "lr_accuracy_top5 = accuracy_score(y_test, lr_model_top5.predict(X_test_top5))\n",
    "\n",
    "# Print the accuracies\n",
    "print('RF Accuracy with Top 5 Features:', rf_accuracy_top5)\n",
    "print('XGB Accuracy with Top 5 Features:', xgb_accuracy_top5)\n",
    "print('LGBM Accuracy with Top 5 Features:', lgbm_accuracy_top5)\n",
    "print('LR Accuracy with Top 5 Features:', lr_accuracy_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f31d5de0-e22f-4dac-9ef2-52aef3ad1b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('random_forest_model_top5.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model_top5, f)\n",
    "with open('xgboost_model_top5.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model_top5, f)\n",
    "with open('lightgbm_model_top5.pkl', 'wb') as f:\n",
    "    pickle.dump(lgbm_model_top5, f)\n",
    "with open('logistic_regression_model_top5.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model_top5, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
